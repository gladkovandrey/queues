package main

import (
	"context"
	"encoding/json"
	"log"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/segmentio/kafka-go"
)

// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ ERROR –ª–æ–≥–∞ (–∏–∑ mapper)
type ErrorLog struct {
	Timestamp   string `json:"timestamp"`
	Service     string `json:"service"`
	Error       string `json:"error"`
	ProcessedAt string `json:"processed_at"`
}

// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞ (–∏–∑ metrics-producer)
type ServiceMetrics struct {
	Timestamp    string  `json:"timestamp"`
	Service      string  `json:"service"`
	CPUUsage     float64 `json:"cpu_usage"`
	MemoryUsage  float64 `json:"memory_usage"`
	LatencyMs    int     `json:"latency_ms"`
	RequestCount int     `json:"request_count"`
	GeneratedAt  string  `json:"generated_at"`
}

// –û–±–æ–≥–∞—â–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –æ—à–∏–±–∫–∞ + –º–µ—Ç—Ä–∏–∫–∏
type EnrichedError struct {
	ErrorLog
	Metrics     *ServiceMetrics `json:"metrics,omitempty"`
	JoinedAt    string          `json:"joined_at"`
	MetricsAge  string          `json:"metrics_age,omitempty"` // –í–æ–∑—Ä–∞—Å—Ç –º–µ—Ç—Ä–∏–∫
}

// –ö—ç—à –º–µ—Ç—Ä–∏–∫ –¥–ª—è join –æ–ø–µ—Ä–∞—Ü–∏–π
type MetricsCache struct {
	mu      sync.RWMutex
	metrics map[string]*ServiceMetrics // –∫–ª—é—á: service
}

func (mc *MetricsCache) Set(service string, metrics *ServiceMetrics) {
	mc.mu.Lock()
	defer mc.mu.Unlock()
	mc.metrics[service] = metrics
}

func (mc *MetricsCache) Get(service string) *ServiceMetrics {
	mc.mu.RLock()
	defer mc.mu.RUnlock()
	return mc.metrics[service]
}

func main() {
	// –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
	servers := getEnvOrDefault("KAFKA_BOOTSTRAP_SERVERS", "localhost:19092")
	errorTopic := getEnvOrDefault("ERROR_TOPIC", "error-logs")
	metricsTopic := getEnvOrDefault("METRICS_TOPIC", "service-metrics")
	outputTopic := getEnvOrDefault("OUTPUT_TOPIC", "enriched-errors")
	consumerGroup := getEnvOrDefault("CONSUMER_GROUP", "join-processor")

	log.Printf("üîó Join Processor –∑–∞–ø—É—â–µ–Ω")
	log.Printf("üì• ERROR –ª–æ–≥–∏ –∏–∑: %s", errorTopic)
	log.Printf("üì• –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑: %s", metricsTopic)
	log.Printf("üì§ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤: %s", outputTopic)

	brokers := strings.Split(servers, ",")

	// –°–æ–∑–¥–∞–µ–º –∫—ç—à –º–µ—Ç—Ä–∏–∫
	cache := &MetricsCache{
		metrics: make(map[string]*ServiceMetrics),
	}

	// –°–æ–∑–¥–∞–µ–º readers
	errorReader := kafka.NewReader(kafka.ReaderConfig{
		Brokers: brokers,
		Topic:   errorTopic,
		GroupID: consumerGroup + "-errors",
		MinBytes: 10e3,
		MaxBytes: 10e6,
	})
	defer errorReader.Close()

	metricsReader := kafka.NewReader(kafka.ReaderConfig{
		Brokers: brokers,
		Topic:   metricsTopic,
		GroupID: consumerGroup + "-metrics",
		MinBytes: 10e3,
		MaxBytes: 10e6,
	})
	defer metricsReader.Close()

	// –°–æ–∑–¥–∞–µ–º writer –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
	writer := kafka.NewWriter(kafka.WriterConfig{
		Brokers: brokers,
		Topic:   outputTopic,
		Balancer: &kafka.LeastBytes{},
	})
	defer writer.Close()

	log.Printf("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

	// –ö–∞–Ω–∞–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
	errorChan := make(chan kafka.Message, 100)
	metricsChan := make(chan kafka.Message, 100)

	// –ì–æ—Ä—É—Ç–∏–Ω–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è ERROR –ª–æ–≥–æ–≤
	go func() {
		for {
			message, err := errorReader.ReadMessage(context.Background())
			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è error: %v", err)
				continue
			}
			errorChan <- message
		}
	}()

	// –ì–æ—Ä—É—Ç–∏–Ω–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
	go func() {
		for {
			message, err := metricsReader.ReadMessage(context.Background())
			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è metrics: %v", err)
				continue
			}
			metricsChan <- message
		}
	}()

	// –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª join –æ–±—Ä–∞–±–æ—Ç–∫–∏
	for {
		select {
		case metricsMsg := <-metricsChan:
			// –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –º–µ—Ç—Ä–∏–∫
			var metrics ServiceMetrics
			err := json.Unmarshal(metricsMsg.Value, &metrics)
			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ JSON metrics: %v", err)
				continue
			}

			cache.Set(metrics.Service, &metrics)
			log.Printf("üìä –û–±–Ω–æ–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è %s: CPU=%.1f%% LAT=%dms",
				metrics.Service, metrics.CPUUsage, metrics.LatencyMs)

		case errorMsg := <-errorChan:
			// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ERROR –ª–æ–≥
			var errorLog ErrorLog
			err := json.Unmarshal(errorMsg.Value, &errorLog)
			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ JSON error: %v", err)
				continue
			}

			// –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
			metrics := cache.Get(errorLog.Service)

			// –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—É—é –∑–∞–ø–∏—Å—å
			enriched := EnrichedError{
				ErrorLog: errorLog,
				Metrics:  metrics,
				JoinedAt: time.Now().Format("2006-01-02 15:04:05"),
			}

			// –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –º–µ—Ç—Ä–∏–∫–∏, –≤—ã—á–∏—Å–ª—è–µ–º –∏—Ö –≤–æ–∑—Ä–∞—Å—Ç
			if metrics != nil {
				metricsTime, err := time.Parse("2006-01-02 15:04:05", metrics.Timestamp)
				if err == nil {
					age := time.Since(metricsTime)
					enriched.MetricsAge = age.String()
				}
			}

			// –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
			enrichedBytes, err := json.Marshal(enriched)
			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %v", err)
				continue
			}

			// –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
			err = writer.WriteMessages(context.Background(), kafka.Message{
				Key:   []byte(errorLog.Service),
				Value: enrichedBytes,
			})

			if err != nil {
				log.Printf("‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏: %v", err)
			} else {
				if metrics != nil {
					log.Printf("üîó JOIN: %s –æ—à–∏–±–∫–∞ + –º–µ—Ç—Ä–∏–∫–∏ (CPU=%.1f%%, LAT=%dms)",
						errorLog.Service, metrics.CPUUsage, metrics.LatencyMs)
				} else {
					log.Printf("üîó JOIN: %s –æ—à–∏–±–∫–∞ –ë–ï–ó –º–µ—Ç—Ä–∏–∫", errorLog.Service)
				}
			}
		}
	}
}

func getEnvOrDefault(key, defaultValue string) string {
	value := os.Getenv(key)
	if value == "" {
		return defaultValue
	}
	return value
} 